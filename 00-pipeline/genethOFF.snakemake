# Guillaume CORRE @ GENETHON 2025
# Snakemake for the processing of GUIDE-seq NGS datasets.

## RUN the pipeline in the project folder.
## snakemake -s PATH/TO/genethOFF.snakemake -k -j 12 --use-conda --quiet -n

###############################################################
# Import python modules
###############################################################

import re
import os
import glob
import pandas as pd
from snakemake.utils import validate
import sys
from collections import defaultdict

from snakemake.utils import min_version
min_version("9.3.0")


RED= "\033[91m"
GREEN= "\033[92m"
RESET= "\033[0m"


###############################################################
# check that the config file is present in current folder
###############################################################
if not os.path.isfile("configuration.yml"):
  print(RED + u'\u274C' + RESET + " Configuration file not found\n")
  sys.exit(1)
else:
  print(GREEN + u'\u2705' + RESET + " Configuration file found\n")
  configfile: "configuration.yml"
  config_file_path=os.getcwd()+'/'+workflow.configfiles[0]



###############################################################
## Load the sample datasheet as a TSV file or excel
###############################################################
if not os.path.isfile(config["sampleInfo_path"]):
  print(RED + u'\u274C' + RESET + " Sample data sheet not found\n")
  sys.exit(1)
else:
  print(GREEN + u'\u2705' + RESET + " Sample data sheet found\n")
  sample_datasheet=config["sampleInfo_path"]

  # if sample_datasheet ends with xlsx,
  if sample_datasheet.endswith("xlsx"):
    samplesTable = pd.read_excel(sample_datasheet).set_index("sampleName", drop=False)
  else:
    samplesTable = pd.read_table(sample_datasheet,sep=";").set_index("sampleName", drop=False)
  
  #check the validity of the sample Data Sheet (Path is relative to pipeline file, not current folder)
  validate(samplesTable, "samples.schema.yaml")




###############################################################
## if skipping demultiplexing and several libraries have the same name, then raise an error
###############################################################

lib_names = samplesTable["sampleName"].tolist()

if config["skip_demultiplexing"] == "TRUE" and len(lib_names)!=len(set(lib_names)):
  print(RED + u'\u274C' + RESET + " All samples name must be unique if demultiplexing is skipped.\n")
  print(samplesTable["sampleName"].to_markdown(index=False),"\n")
  sys.exit(1)

###############################################################
## if skipping demultiplexing , check that libraries folders specified in sample datasheet exist
###############################################################

if config["skip_demultiplexing"] == "TRUE":
  paths = samplesTable["path_to_files"].drop_duplicates().tolist()
  for path in paths:
    if not os.path.isdir(path):
      print(RED + u'\u274C' + " At least one path does not exist in sample datasheet."+ RESET+"\n")
      print(samplesTable[["sampleName","path_to_files"]].to_markdown(index=False),"\n")
      sys.exit(1)



###############################################################
## if multiple rows have the same sampleName, libraries will be merged
#     ---> check that they have the same gRNA,PAM, Cas and genome
#     ---> check for hyphens in sample names to prevent issues with delimiters in file names or identifiers.

###############################################################

columns_to_check = ['sampleName','Genome','gRNA_name', 'gRNA_sequence','PAM_sequence','PAM_side','Cas','type',"Cut_Offset"]

identical_values=samplesTable.groupby(samplesTable.index).apply(lambda x: x[columns_to_check].nunique().eq(1).all())
    
if identical_values.all():
    samples = samplesTable[columns_to_check].drop_duplicates()
    print("#### Samples that will be processed : \n\n",samples.to_markdown(index=False),"\n")
    
    my_list=samples[["Genome","gRNA_sequence","PAM_sequence","PAM_side"]].drop_duplicates()
    
    
    samples_unique=list(set(samples["sampleName"]))
    
    ## check there is no "-" in sample name
    strings_with_hyphen = [item for item in samples_unique if "-" in item]
    if strings_with_hyphen:
        print(f"{RED} !! '-' detected in the following sample name :", strings_with_hyphen,"\n Please modify sample name and replace '-' by another delimiter")
        sys.exit(1)
    else:
      
      genomes=my_list["Genome"].tolist()
      gRNAs=my_list["gRNA_sequence"].tolist()
      PAMs=my_list["PAM_sequence"].tolist()
      sides=my_list["PAM_side"].tolist()
      genomes_unique=list(set(genomes))
 
else:
    print(f"{RED}!! Samples with identical 'sampleName' value must share the same features : gRNA_name, gRNA_sequence, PAM_sequence,PAM_side, Genome, Cas\n---> Please correct metadata or use different samples name{RESET}")
    print(samplesTable[columns_to_check])
    sys.exit(1)


###############################################################
# Check that the protocoles defined in the sample datasheet all have a corresponding entry in the configuration file.
###############################################################

unique_protocols = samplesTable['type'].unique()

yaml_protocols = config.keys()

missing_protocols = [protocol for protocol in unique_protocols if protocol not in yaml_protocols]

if not missing_protocols:
    print(GREEN + u'\u2705' + RESET + " All protocols were found in configuration file \n")
else:
  print(RED + u'\u274C' +  " At least one method is not defined in the configuration file"+ RESET +" \n")
  sys.exit(1)

###############################################################
# Check that the genomes defined in the sample datasheet  all have a corresponding entry in the configuration file.
###############################################################

unique_Genome = samplesTable['Genome'].unique()

yaml_Genome = config["genome"].keys()

missing_Genome = [Genome for Genome in unique_Genome if Genome not in yaml_Genome]

if not missing_Genome:
  print(GREEN + u'\u2705' + RESET + " All reference genomes were found in configuration file \n")
else:
  print(RED + u'\u274C'  + " At least one genome is not defined in the configuration file"+ RESET+" \n")
  sys.exit(1)




###############################################################
# check that the input files are present in indicated folder.
###############################################################

    ###############################################################
    # for undemultiplexed libraries, the files name is define in the CONFIGURATION FILE
    ###############################################################

if config["skip_demultiplexing"] == "FALSE":
  files_to_check = ["R1", "R2", "I1", "I2"]
  
  results = {}
  
  for file_key in files_to_check:
      file_path = os.path.join(config["read_path"], config[file_key])
      if os.path.isfile(file_path):
          results[file_key] =GREEN + u'\u2705' + RESET + " " +file_path 
      else:
          results[file_key] = RED + u'\u274C' + RESET +" "  +file_path 
          
  # Print the results in a table format with colors
  print("## Skip_demultiplexing : FALSE \n")
  print("### Checking if input files are present :\n")

  print(f"      -{results['R1']}")
  print(f"      -{results['R2']}")
  print(f"      -{results['I1']}")
  print(f"      -{results['I2']}\n")
  
  # If any file is missing among R1, R2, I1 and R2, raise an exception
  missing_files = [file_key for file_key, status in results.items() if RED in status]
  if missing_files:
    print(RED + u'\u274C'  + " Some files are missing, see above. Abording ..." + RESET + "\n")
    sys.exit(1)
  else:
    print(GREEN + u'\u2705'  + " All required files were found ... continue processing" + RESET + "\n")


    #########################################################
    ## For already demultiplexed libraries
    ## check that R1, R2 are present in the path defined in the SAMPLE DATASHEET
    ## UMI must be in the R1 and R2 headers in the format : NNNNNNNN+NNNNNNNNNNNNNNNN (8+16 in guideseq, added during demultiplexing from NGS machine)
    ## each fastq file must start with the sample name as defined in the SAMPLE DATASHEET
    ## a symlink from each file is created into 00-demultiplexing so the pipeline  will start after the demultiplexing step
    ########################################################
else:
  print("\n\n ## Skip_demultiplexing : TRUE \n ### Checking if input files are present for each sample :\n")
  
  demultiplexing_dir = "00-demultiplexing"

# Ensure the demultiplexing directory exists
  os.makedirs(demultiplexing_dir, exist_ok=True)

  # Iterate over each sample
  incomplete_samples = 0
  for sample in samples_unique:

    path = samplesTable[samplesTable['sampleName'] == sample]["path_to_files"].drop_duplicates().tolist()[0]
    # Define the patterns for each file type
    patterns = [
      f"{sample}*R1*fastq.gz",
      f"{sample}*R2*fastq.gz",
    ]
    # Check if all files are present for each sample
    all_present = True
    file_paths = []
    for pattern in patterns:
      motif=f"{path}/{pattern}"
      matching_files = glob.glob(motif)
      if not matching_files:
        all_present = False
        file_paths.append(motif)
      else:
        file_paths.append(matching_files[0])
    
      # Print the result
    if all_present:
      print(f"    {sample}")
      for file_path in file_paths:
        print(f"      -" + GREEN+u'\u2705'+RESET +" "+ file_path)
      
      # Create symbolic links for actual path of files to demultiplexing folder
      for file_path, suffix in zip(file_paths, ["R1", "R2"]):
        symlink_path = os.path.join(demultiplexing_dir, f"{sample}_{suffix}.fastq.gz")
        if not os.path.islink(symlink_path):
            os.symlink(file_path, symlink_path)
            
      # check for I2 (if absent, will be generated from R2 read header)
      pattern = f"{sample}*I2*fastq.gz"
      motif=f"{path}/{pattern}"
      matching_files = glob.glob(motif)
      if matching_files:
        print(f"      -" + GREEN+u'\u2705'+RESET +" "+ matching_files[0])
        symlink_path = os.path.join(demultiplexing_dir, f"{sample}_newI2.fastq.gz")
        if not os.path.islink(symlink_path):
          os.symlink(matching_files[0], symlink_path)
      print(GREEN +  "      All required files were found for sample "+sample + RESET + "\n")
    else: #raise an error
      incomplete_samples +=1
      print(f"    {sample}")
      for file_path in file_paths:
        if("*" in file_path):
          print(f"      -" + RED+u'\u274C'+RESET +" "+ file_path)
        else:
          print(f"      -" + GREEN+u'\u2705'+RESET +" "+ file_path)
      print(RED + "      Some files are missing for sample "+sample + RESET + "\n")
    
  if(incomplete_samples > 0):
    print(RED + u'\u274C'  + " Some files are missing. Please check that correct folders are indicated in the sample datasheet below: " + RESET + "\n")
    print("############### List of samples in sample datasheet\n")
    print(samplesTable[["sampleName","path_to_files"]].to_markdown(index=False))
    sys.exit(1)
    
  else:
    print(GREEN + u'\u2705'  + " All files found ... continuing " + RESET + "\n")

  
      

###############################################################
# THIS IS THE WORKFLOW
###############################################################

# make a barcode file with I1 sequences for demultiplexing.
rule make_indexes_fasta_i1:
    input: 
    output: temp("demultiplexing_barcodes_i1.fa")
    run: 
        sample_count = defaultdict(int)
        with open("demultiplexing_barcodes_i1.fa", 'w') as fasta_file_i1:
          samplesTable_sub=samplesTable[["sampleName","index1"]].drop_duplicates()
          for index, row in samplesTable_sub.iterrows():
            sequence_i1 = row['index1']
            sample_count[row['sampleName']] += 1
            sample_header = f">{row['sampleName']}-{sample_count[row['sampleName']]}"
            #sample_header = f">{row['sampleName']}"
            fasta_file_i1.write(sample_header + '\n')
            fasta_file_i1.write(sequence_i1 + '\n')



# demultiplexe libraries based on barcodes in the sampleInfo file and I3 file generated before
## 
# check whether an empty library will make the pip to crash (ie wrong index)
##

rule demux_I1:
    input: 
      R1=os.path.join(config["read_path"], config["R1"]), 
      R2=os.path.join(config["read_path"], config["R2"]), 
      I1=os.path.join(config["read_path"], config["I1"]), 
      I2=os.path.join(config["read_path"], config["I2"]), 
      barcodes=rules.make_indexes_fasta_i1.output
    output: 
      R1=temp(["00-demultiplexing/{sample}-1_i1_R1.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      R2=temp(["00-demultiplexing/{sample}-1_i1_R2.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      I1=temp(["00-demultiplexing/{sample}-1_i1_I1.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      I2=temp(["00-demultiplexing/{sample}-1_i1_I2.fastq.gz".format(sample=sample) for sample in samples["sampleName"]])
    conda: "GENETHOFF"
    log: R1= "logs/demultiplexing_i1_R1.log"
    threads: 6
    shell: """
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action trim --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_R1.fastq.gz {input.I1} {input.R1} > {log.R1}
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action trim --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_R2.fastq.gz {input.I1} {input.R2} > /dev/null
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_I2.fastq.gz {input.I1} {input.I2} > /dev/null

       # rm 00-demultiplexing/*unknown* 
        """
  

rule merge_samples_i1:
  input: 
    R1="00-demultiplexing/{sample}-1_i1_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}-1_i1_R2.fastq.gz", 
    I2="00-demultiplexing/{sample}-1_i1_I2.fastq.gz" 
  output:
    R1=temp("00-demultiplexing/{sample}_i1_R1.fastq.gz"), 
    R2=temp("00-demultiplexing/{sample}_i1_R2.fastq.gz"), 
    I2=temp("00-demultiplexing/{sample}_i1_I2.fastq.gz")
  shell:
    """
      cat 00-demultiplexing/{wildcards.sample}-*_i1_R1.fastq.gz > {output.R1}
      cat 00-demultiplexing/{wildcards.sample}-*_i1_R2.fastq.gz > {output.R2}
      cat 00-demultiplexing/{wildcards.sample}-*_i1_I2.fastq.gz > {output.I2}
    """

rule make_indexes_fasta_i2:
  input: 
  output:temp("demultiplexing_barcodes_i2_{sample}.fa")
  run: 
    sample_count = defaultdict(int)
    with open(output[0], 'w') as fasta_file_i2:
      samplesTable_sub=samplesTable[samplesTable['sampleName'] == wildcards.sample][["sampleName","index2"]].drop_duplicates()
      for index, row in samplesTable_sub.iterrows():
        sequence_i2 = row['index2']
        sample_count[row['sampleName']] += 1
        sample_header = f">{row['sampleName']}-{sample_count[row['sampleName']]}"
        fasta_file_i2.write(sample_header + '\n')
        fasta_file_i2.write(sequence_i2 + '\n')  



rule demux_I2:
  input: 
    R1=rules.merge_samples_i1.output.R1,
    R2=rules.merge_samples_i1.output.R2,
    I2=rules.merge_samples_i1.output.I2,
    barcodes=rules.make_indexes_fasta_i2.output
  output: 
    R1=temp("00-demultiplexing/{sample}-1_i2_R1.fastq.gz"),
    R2=temp("00-demultiplexing/{sample}-1_i2_R2.fastq.gz"),
    I2=temp("00-demultiplexing/{sample}-1_i2_I2.fastq.gz")
  conda: "GENETHOFF"
  log: R1= "logs/demultiplexing_{sample}_i2_R1.log"
  threads: 6
  shell: """
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i2_I2.fastq.gz -p 00-demultiplexing/{{name}}_i2_R1.fastq.gz {input.I2} {input.R1} > {log.R1}
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i2_I2.fastq.gz -p 00-demultiplexing/{{name}}_i2_R2.fastq.gz {input.I2} {input.R2} > /dev/null
        
        #rm 00-demultiplexing/*unknown* 
        """



rule merge_samples_i2:
  input: 
    R1=rules.demux_I2.output.R1,
    R2=rules.demux_I2.output.R2,
    I2=rules.demux_I2.output.I2 
  output:
    R1="00-demultiplexing/{sample}_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}_R2.fastq.gz", 
    I2="00-demultiplexing/{sample}_I2.fastq.gz"
  shell:
    """
      cat 00-demultiplexing/{wildcards.sample}-*_i2_R1.fastq.gz > {output.R1}
      cat 00-demultiplexing/{wildcards.sample}-*_i2_R2.fastq.gz > {output.R2}
      cat 00-demultiplexing/{wildcards.sample}-*_i2_I2.fastq.gz > {output.I2}
    """

# make an I2 file : extract UMI from read header
rule make_I2:
  input:
    R1="00-demultiplexing/{sample}_R1.fastq.gz"
  output:
    temp("00-demultiplexing/{sample}_newI2.fastq.gz")
  threads: 1
  conda: "GENETHOFF"
  shell: """
    python $HOME"/research_code/GENETHOFF/03-misc/extract_I2_from_header.py" {input} {output}
  """
  

# Add UMI to read name
rule add_UMI:
  input: 
    I2=branch(lambda wildcards: (
            str(lookup(dpath="skip_demultiplexing", within=config)).lower() == "true" and not os.path.islink("00-demultiplexing/{sample}_newI2.fastq.gz")) ,
            then=rules.make_I2.output,
            otherwise="00-demultiplexing/{sample}_I2.fastq.gz"
        ),
    R1="00-demultiplexing/{sample}_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}_R2.fastq.gz"
  output: 
    R1=temp("01-trimming/{sample}_R1.UMI.fastq.gz"),
    R2=temp("01-trimming/{sample}_R2.UMI.fastq.gz"),
    I2=temp("01-trimming/{sample}_I2.UMI.fastq.gz")
  threads:6
  log: R1="logs/{sample}_R1.UMI.log", R2="logs/{sample}_R2.UMI.log"    
  conda: "GENETHOFF"
  params:
    UMI=config["UMI_pattern"], ## UMI pattern
    UMI_side=config["UMI_side"] ## either 5 (left) or 3 (right)
  shell: """
        UMI_length=$(expr length {params.UMI})
        
        if [ "{params.UMI_side}" = "5" ]; then
          cutadapt -j {threads} -u $UMI_length --rename='{{id}}_{{r1.cut_prefix}} {{comment}}' -o {output.I2} -p {output.R1} {input.I2} {input.R1} > {log.R1}
          cutadapt -j {threads} -u $UMI_length --rename='{{id}}_{{r1.cut_prefix}} {{comment}}' -o {output.I2} -p {output.R2} {input.I2} {input.R2} > {log.R2}
        else
          cutadapt -j {threads} -u -$UMI_length --rename='{{id}}_{{r1.cut_suffix}} {{comment}}' -o {output.I2} -p {output.R1} {input.I2} {input.R1} > {log.R1}
          cutadapt -j {threads} -u -$UMI_length --rename='{{id}}_{{r1.cut_suffix}} {{comment}}' -o {output.I2} -p {output.R2} {input.I2} {input.R2} > {log.R2}
        fi 
        """

# remove ODN and discard reads without ODN
rule trim_ODN:
  input: 
    R1=rules.add_UMI.output.R1,
    R2=rules.add_UMI.output.R2
  output: 
    R1=temp("01-trimming/{sample}_R1.ODN.UMI.fastq.gz"),
    R2=temp("01-trimming/{sample}_R2.ODN.UMI.fastq.gz")
  threads: 6
  log:R1="logs/{sample}_R1.odn.log"
  conda: "GENETHOFF"
  message: "removing ODN sequence, discard reads without ODN sequence {wildcards.sample}"
  params: ODN_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R2_leading"],
    ODN_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R2_leading"]
  shell: """
        cutadapt -j {threads} -G "negative={params.ODN_neg};max_error_rate=0;rightmost" -G "positive={params.ODN_pos};max_error_rate=0;rightmost" --discard-untrimmed  --rename='{{id}}_{{r2.adapter_name}} {{comment}}' -o {output.R1} -p {output.R2} {input.R1} {input.R2} > {log.R1}
         """


  # remove leading and trailing ODN sequences
rule trim_reads:
  input: 
    R1=rules.trim_ODN.output.R1, 
    R2=rules.trim_ODN.output.R2
  output: 
    R1=temp("01-trimming/{sample}_R1.ODN.UMI.trimmed.fastq.gz"), 
    R2=temp("01-trimming/{sample}_R2.ODN.UMI.trimmed.fastq.gz")
  threads: 6
  log: "logs/{sample}.trailing.log"
  conda: "GENETHOFF"
  message: "trimming ODN and adaptor sequences in reads"
  params: 
    R2_trailing_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R2_trailing"],
    R2_trailing_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R2_trailing"],
    R1_trailing_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R1_trailing"],
    R1_trailing_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R1_trailing"]
  shell: """
        cutadapt -j {threads} \
          -A "{params.R2_trailing_pos};min_overlap=6;max_error_rate=0.1" \
          -A "{params.R2_trailing_neg};min_overlap=6;max_error_rate=0.1" \
          -a "{params.R1_trailing_pos};min_overlap=6;max_error_rate=0.1" \
          -a "{params.R1_trailing_neg};min_overlap=6;max_error_rate=0.1" \
          -o {output.R1} -p {output.R2} {input.R1} {input.R2} > {log}
        """


  # make a size selection before mapping
rule filter_reads:
  input: 
    R1=rules.trim_reads.output.R1, 
    R2=rules.trim_reads.output.R2
  output:  
    R1="02-filtering/{sample}_R1.UMI.ODN.trimmed.filtered.fastq.gz", 
    R2="02-filtering/{sample}_R2.UMI.ODN.trimmed.filtered.fastq.gz",
    R1short=temp("02-filtering/{sample}_R1.UMI.ODN.trimmed.tooshort.fastq.gz"), 
    R2short=temp("02-filtering/{sample}_R2.UMI.ODN.trimmed.tooshort.fastq.gz")
  threads: 6
  log: "logs/{sample}.filter.log"
  params: length=config["minLength"]
  conda: "GENETHOFF"
  message: "remove pairs if one mate is shorter than x bp"
  shell: """
        cutadapt -j {threads} \
          --pair-filter=any \
          --minimum-length {params.length} \
          --too-short-output {output.R1short} \
          --too-short-paired-output  {output.R2short} \
          --output {output.R1} \
          --paired-output {output.R2} {input.R1} {input.R2} > {log}
        """


  # map reads on the reference genome as pairs
if (config["aligner"]  == "bowtie2" or config["aligner"]  == "Bowtie2") :
    rule alignOnGenome:
        input: 
          R1=rules.filter_reads.output.R1,
          R2=rules.filter_reads.output.R2
        output: 
          sam=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sam"),
          R2_unmapped="03-align/{sample}_R2.UMI.ODN.trimmed.unmapped.fastq.gz"
        threads: 6
        log: "logs/{sample}.UMI.ODN.trimmed.filtered.align.log"
        conda: "GENETHOFF"
        message: "Aligning PE reads on genome with Bowtie2"
        params: 
          index=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["index"],
          minFragLength=config["minFragLength"],
          maxFragLength=config["maxFragLength"]
        shell: """
                bowtie2 -p {threads} --no-unal \
                  -I {params.minFragLength} \
                  -X {params.maxFragLength} \
                  --dovetail \
                  --no-mixed \
                  --no-discordant \
                  --un-conc-gz 03-align/{wildcards.sample}_R%.UMI.ODN.trimmed.unmapped.fastq.gz   \
                  -x {params.index} \
                  -1 {input.R1} -2 {input.R2} -S {output.sam} 2> {log}
            """

elif (config["aligner"]  == "bwa") :
    rule alignOnGenome:
        input: 
          R1=rules.filter_reads.output.R1,
          R2=rules.filter_reads.output.R2
        output: 
          sam=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sam"),
          unfilterdsam=temp("03-align/{sample}.UMI.ODN.trimmed.unfiltered.sam")
        threads: 6
        log: "logs/{sample}.UMI.ODN.trimmed.filtered.align.log"
        conda: "GENETHOFF"
        message: "Aligning PE reads on genome with BWA mem"
        params:  index=config["genome"][lambda wildcards:samples["Genome"][wildcards.sample]]["index"]
        shell: """
                #bwa mem -t {threads} {params.index} {input.R1} {input.R2} > {output.sam} 2> {log}
                bwa mem -t {threads} {params.index} {input.R1} {input.R2} > {output.unfilterdsam} 2> {log}
                samtools view -F 0x4 -F 0x8 -F 0x100 -F 0x800 -f 0x2 -b {output.unfilterdsam} > {output.sam}
            """
  
  ## true multihits have MAPQ=1 with bowtie2
  
rule filter_alignments:
    input: 
      sam=rules.alignOnGenome.output.sam
    output: 
      list=temp("03-align/{sample}_multi.txt"),
    threads: 2
    conda: "GENETHOFF"
    message: "keep only alignments with high MAPQ or with equal score with best secondary alignment (multihits)"
    params: minMAPQ=config["minMAPQ"]
    shell: """
    # https://biofinysics.blogspot.com/2014/05/how-does-bowtie2-assign-mapq-scores.html#bt2expt 
        samtools view  {input} -e '((mapq ==1 && [AS] == [XS]) || (mapq >={params.minMAPQ}) || (mapq == 6))' | cut -f1 | sort | uniq  > {output.list}
    """



# sort alignments by names (required for BEDPE conversion) and position (for viewing)
rule sort_aligned:
    input: 
      sam=rules.alignOnGenome.output.sam, 
      list = rules.filter_alignments.output.list
    output:  
      bam="03-align/{sample}.UMI.ODN.trimmed.filtered.sorted.filtered.bam",
      bamPos=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sorted.bam"),
      bamName=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sortedName.filtered.bam")
    threads: 6
    conda: "GENETHOFF"
    message: "Sort reads by name"
    shell: """
        samtools sort -@ {threads} {input.sam}  > {output.bamPos}

        samtools view -hb --qname-file {input.list}  {output.bamPos} > {output.bam}
        samtools index {output.bam}
        samtools sort -n -@ {threads} {output.bam} > {output.bamName}
    """



# convert BAM file to BEDPE and identify insertion point, read length
# filter alignment with MAPQ score > threshold in config file
# aggregate reads per fragment size and genomic coordinates
# add a cluster ID to group close IS (distance defined in the config file) 
rule call_IS:
    input: 
      rules.sort_aligned.output.bamName
    output: 
      tmp=temp("04-IScalling/{sample}.pebed"), 
      umi="04-IScalling/{sample}.reads_per_UMI_per_IS.bed"
    threads: 1
    conda: "GENETHOFF"
    params: UMI=config["UMI_pattern"]
    shell: """
    
        UMI_length=$(expr length {params.UMI})

        bedtools bamtobed -bedpe -mate1 -i {input} > {output.tmp}
        
        # read R2 contains the genome/ODN junction (use $10 of PE bed = mate 2)
        
        ##################################################################
        # count number of reads per UMI and per IS (pos and strand separated)
        ##################################################################

        awk 'BEGIN{{OFS="\\t";FS="\\t"}} ($1 == $4) {{split($7,a,"_"); if($10=="+") print $4,$5,$5,a[1],a[2],a[3],$8,$10,$3-$5; else print $4,$6-1,$6-1,a[1],a[2],a[3],$8,$10,$6-$2}}' {output.tmp} | sort -k1,1 -k2,3n -k6,6 -k5,5 -k8,8 | bedtools groupby -g 1,2,3,6,5,8 -c 4,7 -o count_distinct,median  > {output.umi}
        
        """


############ RESCUE R2 READS ###################
rule rescue_R2:
  input: 
   R2_short=rules.filter_reads.output.R2short,
   R2_unmapped=rules.alignOnGenome.output.R2_unmapped
  output: 
    sam=temp("03-align/{sample}_R2rescued.UMI.ODN.trimmed.filtered.sam")
  threads: 6
  conda: "GENETHOFF"
  log: "logs/{sample}_R2rescued.UMI.ODN.trimmed.filtered.align.log"
  params: index=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["index"]
  shell: """
    bowtie2 -p {threads} --no-unal \
    -x {params.index} \
    -U {input.R2_short} {input.R2_unmapped} -S {output.sam} 2> {log}
      """

rule sort_rescued_R2:
  input: rules.rescue_R2.output
  output: "03-align/{sample}_R2rescued.UMI.ODN.trimmed.filtered.sorted.bam"
  conda: "GENETHOFF"
  threads:6
  params: minMAPQ=config["minMAPQ"]
  shell: """
    samtools view -b -h  {input} -e '((mapq ==1 && [AS] == [XS]) || (mapq >={params.minMAPQ}))' | samtools sort - > {output}
    samtools index {output}
  """

rule callIS_R2rescued:
  input: rules.sort_rescued_R2.output
  output: 
    umi="04-IScalling/{sample}_R2rescued.reads_per_UMI_per_IS.bed", 
    tmp=temp("04-IScalling/{sample}_rescueR2.bed")
  threads: 1
  conda: "GENETHOFF"
  params: UMI=config["UMI_pattern"]
  shell: """
  
      UMI_length=$(expr length {params.UMI})

      bedtools bamtobed -i {input} > {output.tmp}
      
      # read R2 contains the genome/ODN junction
      
      ##################################################################
      # count number of reads per UMI and per IS (pos and strand separated)
      ##################################################################

      awk 'BEGIN{{OFS="\\t";FS="\\t"}} {{split($4,a,"_"); if($6=="+") print $1,$2,$2,a[1],a[2],a[3],$5,$6,$3-$2; else print $1,$3-1,$3-1,a[1],a[2],a[3],$5,$6,$3-$2}}' {output.tmp} | sort -k1,1 -k2,3n -k6,6 -k5,5 -k8,8 | bedtools groupby -g 1,2,3,6,5,8 -c 4,7 -o count_distinct,median  > {output.umi}
      
      """

rule correct_UMI:
    input: 
      bed=rules.call_IS.output.umi, 
      bed_rescued=rules.callIS_R2rescued.output.umi
    output: 
      "04-IScalling/{sample}.reads_per_UMI_per_IS_corrected.bed"
    conda: "GENETHOFF"
    threads: 2
    params: hamming=config["UMI_hamming_distance"], filter = config["UMI_filter"],  UMI=config["UMI_pattern"], method = config["UMI_deduplication"], rescueR2=config["rescue_R2"]
    shell: """
        Rscript $HOME"/research_code/GENETHOFF/00-pipeline/correct_umi.R" {input.bed} {params.UMI} {params.filter} {params.hamming} {params.method} {output} {params.rescueR2}
        """

rule collapse_umis:
  input: 
    rules.correct_UMI.output
  output:
    "04-IScalling/umi_table{sample}.rds"
  conda: "GENETHOFF"
  threads: 2
  shell: """
    Rscript $HOME"/research_code/GENETHOFF/00-pipeline/collapse_umis.R" {input} {output}
  """
  
rule all:
    input:
        expand("04-IScalling/umi_table{sample}.rds", sample=samples_unique)
