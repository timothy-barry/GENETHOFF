# Guillaume CORRE @ GENETHON 2025
# Snakemake for the processing of GUIDE-seq NGS datasets.

## RUN the pipeline in the project folder.
## snakemake -s PATH/TO/genethOFF.snakemake -k -j 12 --use-conda --quiet -n

###############################################################
# Import python modules
###############################################################

import re
import os
import glob
import pandas as pd
from snakemake.utils import validate
import sys
from collections import defaultdict

from snakemake.utils import min_version
min_version("9.3.0")


RED= "\033[91m"
GREEN= "\033[92m"
RESET= "\033[0m"


os.system("cat ../03-misc/logo.txt; echo '\n'")

###############################################################
# check that the config file is present in current folder
###############################################################
if not os.path.isfile("configuration.yml"):
  print(RED + u'\u274C' + RESET + " Configuration file not found\n")
  sys.exit(1)
else:
  print(GREEN + u'\u2705' + RESET + " Configuration file found\n")
  configfile: "configuration.yml"
  config_file_path=os.getcwd()+'/'+workflow.configfiles[0]



###############################################################
## Load the sample datasheet as a TSV file or excel
###############################################################
if not os.path.isfile(config["sampleInfo_path"]):
  print(RED + u'\u274C' + RESET + " Sample data sheet not found\n")
  sys.exit(1)
else:
  print(GREEN + u'\u2705' + RESET + " Sample data sheet found\n")
  sample_datasheet=config["sampleInfo_path"]

  # if sample_datasheet ends with xlsx,
  if sample_datasheet.endswith("xlsx"):
    samplesTable =pd.read_excel(sample_datasheet).set_index("sampleName", drop=False)
  else:
    samplesTable = pd.read_table(sample_datasheet,sep=";").set_index("sampleName", drop=False)
  
  #check the validity of the sample Data Sheet (Path is relative to pipeline file, not current folder)
  validate(samplesTable, "samples.schema.yaml")




###############################################################
## if skipping demultiplexing and several libraries have the same name, then raise an error
###############################################################

lib_names = samplesTable["sampleName"].tolist()

if config["skip_demultiplexing"] == "TRUE" and len(lib_names)!=len(set(lib_names)):
  print(RED + u'\u274C' + RESET + " All samples name must be unique if demultiplexing is skipped.\n")
  print(samplesTable["sampleName"].to_markdown(index=False),"\n")
  sys.exit(1)

###############################################################
## if skipping demultiplexing , check that libraries folders specified in sample datasheet exist
###############################################################

if config["skip_demultiplexing"] == "TRUE":
  paths = samplesTable["path_to_files"].drop_duplicates().tolist()
  for path in paths:
    if not os.path.isdir(path):
      print(RED + u'\u274C' + " At least one path does not exist in sample datasheet."+ RESET+"\n")
      print(samplesTable[["sampleName","path_to_files"]].to_markdown(index=False),"\n")
      sys.exit(1)



###############################################################
## if multiple rows have the same sampleName, libraries will be merged
#     ---> check that they have the same gRNA,PAM, Cas and genome
#     ---> check for hyphens in sample names to prevent issues with delimiters in file names or identifiers.

###############################################################

columns_to_check = ['sampleName','Genome','gRNA_name', 'gRNA_sequence','PAM_sequence','PAM_side','Cas','type',"Cut_Offset"]

identical_values=samplesTable.groupby(samplesTable.index).apply(lambda x: x[columns_to_check].nunique().eq(1).all())
    
if identical_values.all():
    samples = samplesTable[columns_to_check].drop_duplicates()
    print("#### Samples that will be processed : \n\n",samples.to_markdown(index=False),"\n")
    
    my_list=samples[["Genome","gRNA_sequence","PAM_sequence","PAM_side"]].drop_duplicates()
    
    
    samples_unique=list(set(samples["sampleName"]))
    
    ## check there is no "-" in sample name
    strings_with_hyphen = [item for item in samples_unique if "-" in item]
    if strings_with_hyphen:
        print(f"{RED} !! '-' detected in the following sample name :", strings_with_hyphen,"\n Please modify sample name and replace '-' by another delimiter")
        sys.exit(1)
    else:
      
      genomes=my_list["Genome"].tolist()
      gRNAs=my_list["gRNA_sequence"].tolist()
      PAMs=my_list["PAM_sequence"].tolist()
      sides=my_list["PAM_side"].tolist()
      genomes_unique=list(set(genomes))
 
else:
    print(f"{RED}!! Samples with identical 'sampleName' value must share the same features : gRNA_name, gRNA_sequence, PAM_sequence,PAM_side, Genome, Cas\n---> Please correct metadata or use different samples name{RESET}")
    print(samplesTable[columns_to_check])
    sys.exit(1)


###############################################################
# Check that the protocoles defined in the sample datasheet all have a corresponding entry in the configuration file.
###############################################################

unique_protocols = samplesTable['type'].unique()

yaml_protocols = config.keys()

missing_protocols = [protocol for protocol in unique_protocols if protocol not in yaml_protocols]

if not missing_protocols:
    print(GREEN + u'\u2705' + RESET + " All protocols were found in configuration file \n")
else:
  print(RED + u'\u274C' +  " At least one method is not defined in the configuration file"+ RESET +" \n")
  sys.exit(1)

###############################################################
# Check that the genomes defined in the sample datasheet  all have a corresponding entry in the configuration file.
###############################################################

unique_Genome = samplesTable['Genome'].unique()

yaml_Genome = config["genome"].keys()

missing_Genome = [Genome for Genome in unique_Genome if Genome not in yaml_Genome]

if not missing_Genome:
  print(GREEN + u'\u2705' + RESET + " All reference genomes were found in configuration file \n")
else:
  print(RED + u'\u274C'  + " At least one genome is not defined in the configuration file"+ RESET+" \n")
  sys.exit(1)




###############################################################
# check that the input files are present in indicated folder.
###############################################################

    ###############################################################
    # for undemultiplexed libraries, the files name is define in the CONFIGURATION FILE
    ###############################################################

if config["skip_demultiplexing"] == "FALSE":
  files_to_check = ["R1", "R2", "I1", "I2"]
  
  results = {}
  
  for file_key in files_to_check:
      file_path = os.path.join(config["read_path"], config[file_key])
      if os.path.isfile(file_path):
          results[file_key] =GREEN + u'\u2705' + RESET + " " +file_path 
      else:
          results[file_key] = RED + u'\u274C' + RESET +" "  +file_path 
          
  # Print the results in a table format with colors
  print("## Skip_demultiplexing : FALSE \n")
  print("### Checking if input files are present :\n")

  print(f"      -{results['R1']}")
  print(f"      -{results['R2']}")
  print(f"      -{results['I1']}")
  print(f"      -{results['I2']}\n")
  
  # If any file is missing among R1, R2, I1 and R2, raise an exception
  missing_files = [file_key for file_key, status in results.items() if RED in status]
  if missing_files:
    print(RED + u'\u274C'  + " Some files are missing, see above. Abording ..." + RESET + "\n")
    sys.exit(1)
  else:
    print(GREEN + u'\u2705'  + " All required files were found ... continue processing" + RESET + "\n")


    #########################################################
    ## For already demultiplexed libraries
    ## check that R1, R2 are present in the path defined in the SAMPLE DATASHEET
    ## UMI must be in the R1 and R2 headers in the format : NNNNNNNN+NNNNNNNNNNNNNNNN (8+16 in guideseq, added during demultiplexing from NGS machine)
    ## each fastq file must start with the sample name as defined in the SAMPLE DATASHEET
    ## a symlink from each file is created into 00-demultiplexing so the pipeline  will start after the demultiplexing step
    ########################################################
else:
  print("\n\n ## Skip_demultiplexing : TRUE \n ### Checking if input files are present for each sample :\n")
  
  demultiplexing_dir = "00-demultiplexing"

# Ensure the demultiplexing directory exists
  os.makedirs(demultiplexing_dir, exist_ok=True)

  # Iterate over each sample
  incomplete_samples = 0
  for sample in samples_unique:

    path = samplesTable[samplesTable['sampleName'] == sample]["path_to_files"].drop_duplicates().tolist()[0]
    # Define the patterns for each file type
    patterns = [
      f"{sample}*R1*fastq.gz",
      f"{sample}*R2*fastq.gz",
    ]
    # Check if all files are present for each sample
    all_present = True
    file_paths = []
    for pattern in patterns:
      motif=f"{path}/{pattern}"
      matching_files = glob.glob(motif)
      if not matching_files:
        all_present = False
        file_paths.append(motif)
      else:
        file_paths.append(matching_files[0])
    
      # Print the result
    if all_present:
      print(f"    {sample}")
      for file_path in file_paths:
        print(f"      -" + GREEN+u'\u2705'+RESET +" "+ file_path)
      
      # Create symbolic links for actual path of files to demultiplexing folder
      for file_path, suffix in zip(file_paths, ["R1", "R2"]):
        symlink_path = os.path.join(demultiplexing_dir, f"{sample}_{suffix}.fastq.gz")
        if not os.path.islink(symlink_path):
            os.symlink(file_path, symlink_path)
            
      # check for I2 (if absent, will be generated from R2 read header)
      pattern = f"{sample}*I2*fastq.gz"
      motif=f"{path}/{pattern}"
      matching_files = glob.glob(motif)
      if matching_files:
        print(f"      -" + GREEN+u'\u2705'+RESET +" "+ matching_files[0])
        symlink_path = os.path.join(demultiplexing_dir, f"{sample}_newI2.fastq.gz")
        if not os.path.islink(symlink_path):
          os.symlink(matching_files[0], symlink_path)
      print(GREEN +  "      All required files were found for sample "+sample + RESET + "\n")
    else: #raise an error
      incomplete_samples +=1
      print(f"    {sample}")
      for file_path in file_paths:
        if("*" in file_path):
          print(f"      -" + RED+u'\u274C'+RESET +" "+ file_path)
        else:
          print(f"      -" + GREEN+u'\u2705'+RESET +" "+ file_path)
      print(RED + "      Some files are missing for sample "+sample + RESET + "\n")
    
  if(incomplete_samples > 0):
    print(RED + u'\u274C'  + " Some files are missing. Please check that correct folders are indicated in the sample datasheet below: " + RESET + "\n")
    print("############### List of samples in sample datasheet\n")
    print(samplesTable[["sampleName","path_to_files"]].to_markdown(index=False))
    sys.exit(1)
    
  else:
    print(GREEN + u'\u2705'  + " All files found ... continuing " + RESET + "\n")

  
      

###############################################################
# THIS IS THE WORKFLOW
###############################################################

rule stop_at_collapse:
    input:
        expand("04-IScalling/{sample}.UMIs_per_IS_in_Cluster.bed", sample=samples_unique)

rule target:
    input:
     ["results/{sample}_summary.xlsx".format(sample=sample) for sample in samples_unique],
     "results/"+os.path.basename(os.getcwd())+"_report.html"
     

rule get_chrom_length:
    input: lambda wildcards: config["genome"][wildcards.genome]["fasta"]
    output: "../02-ressources/{genome}.chrom.length"
    threads: 1
    conda: "GENETHOFF"
    params: lambda wildcards: config["genome"][wildcards.genome]["fasta"]+".fai"
    shell: """
        if [ ! -e {params} ]
        then
          samtools faidx {input}  ## you must have write access to this path
        fi
        
        ln -s  {params} {output}
        """


rule prepare_annotations:
    input: lambda wildcards: config["genome"][wildcards.genome]["annotation"]
    output: "../02-ressources/{genome}.rds"
    threads: 1
    conda: "GENETHOFF"
    shell: """
        Rscript ../00-pipeline/prepare_annotations.R {wildcards.genome} {input}
    """     

# make a barcode file with I1 sequences for demultiplexing.
rule make_indexes_fasta_i1:
    input: 
    output: temp("demultiplexing_barcodes_i1.fa")
    run: 
        sample_count = defaultdict(int)
        with open("demultiplexing_barcodes_i1.fa", 'w') as fasta_file_i1:
          samplesTable_sub=samplesTable[["sampleName","index1"]].drop_duplicates()
          for index, row in samplesTable_sub.iterrows():
            sequence_i1 = row['index1']
            sample_count[row['sampleName']] += 1
            sample_header = f">{row['sampleName']}-{sample_count[row['sampleName']]}"
            #sample_header = f">{row['sampleName']}"
            fasta_file_i1.write(sample_header + '\n')
            fasta_file_i1.write(sequence_i1 + '\n')



# demultiplexe libraries based on barcodes in the sampleInfo file and I3 file generated before
## 
# check whether an empty library will make the pip to crash (ie wrong index)
##

rule demux_I1:
    input: 
      R1=os.path.join(config["read_path"], config["R1"]), 
      R2=os.path.join(config["read_path"], config["R2"]), 
      I1=os.path.join(config["read_path"], config["I1"]), 
      I2=os.path.join(config["read_path"], config["I2"]), 
      barcodes=rules.make_indexes_fasta_i1.output
    output: 
      R1=temp(["00-demultiplexing/{sample}-1_i1_R1.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      R2=temp(["00-demultiplexing/{sample}-1_i1_R2.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      I1=temp(["00-demultiplexing/{sample}-1_i1_I1.fastq.gz".format(sample=sample) for sample in samples["sampleName"]]),
      I2=temp(["00-demultiplexing/{sample}-1_i1_I2.fastq.gz".format(sample=sample) for sample in samples["sampleName"]])
    conda: "GENETHOFF"
    log: R1= "logs/demultiplexing_i1_R1.log"
    threads: 6
    shell: """
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action trim --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_R1.fastq.gz {input.I1} {input.R1} > {log.R1}
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action trim --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_R2.fastq.gz {input.I1} {input.R2} > /dev/null
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i1_I1.fastq.gz -p 00-demultiplexing/{{name}}_i1_I2.fastq.gz {input.I1} {input.I2} > /dev/null

       # rm 00-demultiplexing/*unknown* 
        """
  

rule merge_samples_i1:
  input: 
    R1="00-demultiplexing/{sample}-1_i1_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}-1_i1_R2.fastq.gz", 
    I2="00-demultiplexing/{sample}-1_i1_I2.fastq.gz" 
  output:
    R1=temp("00-demultiplexing/{sample}_i1_R1.fastq.gz"), 
    R2=temp("00-demultiplexing/{sample}_i1_R2.fastq.gz"), 
    I2=temp("00-demultiplexing/{sample}_i1_I2.fastq.gz")
  shell:
    """
      cat 00-demultiplexing/{wildcards.sample}-*_i1_R1.fastq.gz > {output.R1}
      cat 00-demultiplexing/{wildcards.sample}-*_i1_R2.fastq.gz > {output.R2}
      cat 00-demultiplexing/{wildcards.sample}-*_i1_I2.fastq.gz > {output.I2}
    """

rule make_indexes_fasta_i2:
  input: 
  output:temp("demultiplexing_barcodes_i2_{sample}.fa")
  run: 
    sample_count = defaultdict(int)
    with open(output[0], 'w') as fasta_file_i2:
      samplesTable_sub=samplesTable[samplesTable['sampleName'] == wildcards.sample][["sampleName","index2"]].drop_duplicates()
      for index, row in samplesTable_sub.iterrows():
        sequence_i2 = row['index2']
        sample_count[row['sampleName']] += 1
        sample_header = f">{row['sampleName']}-{sample_count[row['sampleName']]}"
        fasta_file_i2.write(sample_header + '\n')
        fasta_file_i2.write(sequence_i2 + '\n')  



rule demux_I2:
  input: 
    R1=rules.merge_samples_i1.output.R1,
    R2=rules.merge_samples_i1.output.R2,
    I2=rules.merge_samples_i1.output.I2,
    barcodes=rules.make_indexes_fasta_i2.output
  output: 
    R1=temp("00-demultiplexing/{sample}-1_i2_R1.fastq.gz"),
    R2=temp("00-demultiplexing/{sample}-1_i2_R2.fastq.gz"),
    I2=temp("00-demultiplexing/{sample}-1_i2_I2.fastq.gz")
  conda: "GENETHOFF"
  log: R1= "logs/demultiplexing_{sample}_i2_R1.log"
  threads: 6
  shell: """
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i2_I2.fastq.gz -p 00-demultiplexing/{{name}}_i2_R1.fastq.gz {input.I2} {input.R1} > {log.R1}
        
        cutadapt -g ^file:{input.barcodes} -j {threads} -e 0 --discard-untrimmed --action none --no-indels -o 00-demultiplexing/{{name}}_i2_I2.fastq.gz -p 00-demultiplexing/{{name}}_i2_R2.fastq.gz {input.I2} {input.R2} > /dev/null
        
        #rm 00-demultiplexing/*unknown* 
        """



rule merge_samples_i2:
  input: 
    R1=rules.demux_I2.output.R1,
    R2=rules.demux_I2.output.R2,
    I2=rules.demux_I2.output.I2 
  output:
    R1="00-demultiplexing/{sample}_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}_R2.fastq.gz", 
    I2="00-demultiplexing/{sample}_I2.fastq.gz"
  shell:
    """
      cat 00-demultiplexing/{wildcards.sample}-*_i2_R1.fastq.gz > {output.R1}
      cat 00-demultiplexing/{wildcards.sample}-*_i2_R2.fastq.gz > {output.R2}
      cat 00-demultiplexing/{wildcards.sample}-*_i2_I2.fastq.gz > {output.I2}
    """

# make an I2 file : extract UMI from read header
rule make_I2:
  input:
    R1="00-demultiplexing/{sample}_R1.fastq.gz"
  output:
    temp("00-demultiplexing/{sample}_newI2.fastq.gz")
  threads: 1
  conda: "GENETHOFF"
  shell: """
    python ../03-misc/extract_I2_from_header.py {input} {output}
  """
  

  
# Add UMI to read name
rule add_UMI:
  input: 
    I2=branch(lambda wildcards: (
            str(lookup(dpath="skip_demultiplexing", within=config)).lower() == "true" and not os.path.islink("00-demultiplexing/{sample}_newI2.fastq.gz")) ,
            then=rules.make_I2.output,
            otherwise="00-demultiplexing/{sample}_I2.fastq.gz"
        ),
    R1="00-demultiplexing/{sample}_R1.fastq.gz", 
    R2="00-demultiplexing/{sample}_R2.fastq.gz"
  output: 
    R1=temp("01-trimming/{sample}_R1.UMI.fastq.gz"),
    R2=temp("01-trimming/{sample}_R2.UMI.fastq.gz"),
    I2=temp("01-trimming/{sample}_I2.UMI.fastq.gz")
  threads:6
  log: R1="logs/{sample}_R1.UMI.log", R2="logs/{sample}_R2.UMI.log"    
  conda: "GENETHOFF"
  params: UMI=config["UMI_pattern"] ## bp in 3' of index to considere as UMI
  shell: """
        UMI_length=$(expr length {params.UMI})
        
        cutadapt -j {threads} -u -$UMI_length --rename='{{id}}_{{r1.cut_suffix}} {{comment}}' -o {output.I2} -p {output.R1} {input.I2} {input.R1} > {log.R1}
        cutadapt -j {threads} -u -$UMI_length --rename='{{id}}_{{r1.cut_suffix}} {{comment}}' -o {output.I2} -p {output.R2} {input.I2} {input.R2} > {log.R2}
        """



# remove ODN and discard reads without ODN
rule trim_ODN:
  input: 
    R1=rules.add_UMI.output.R1,
    R2=rules.add_UMI.output.R2
  output: 
    R1=temp("01-trimming/{sample}_R1.ODN.UMI.fastq.gz"),
    R2=temp("01-trimming/{sample}_R2.ODN.UMI.fastq.gz")
  threads: 6
  log:R1="logs/{sample}_R1.odn.log"
  conda: "GENETHOFF"
  message: "removing ODN sequence, discard reads without ODN sequence {wildcards.sample}"
  params: ODN_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R2_leading"],
    ODN_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R2_leading"]
  shell: """
        cutadapt -j {threads} -G "negative={params.ODN_neg};max_error_rate=0;rightmost" -G "positive={params.ODN_pos};max_error_rate=0;rightmost" --discard-untrimmed  --rename='{{id}}_{{r2.adapter_name}} {{comment}}' -o {output.R1} -p {output.R2} {input.R1} {input.R2} > {log.R1}
        

          """


  # remove leading and trailing ODN sequences
rule trim_reads:
  input: 
    R1=rules.trim_ODN.output.R1, 
    R2=rules.trim_ODN.output.R2
  output: 
    R1=temp("01-trimming/{sample}_R1.ODN.UMI.trimmed.fastq.gz"), 
    R2=temp("01-trimming/{sample}_R2.ODN.UMI.trimmed.fastq.gz")
  threads: 6
  log: "logs/{sample}.trailing.log"
  conda: "GENETHOFF"
  message: "trimming ODN and adaptor sequences in reads"
  params: 
    R2_trailing_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R2_trailing"],
    R2_trailing_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R2_trailing"],
    R1_trailing_pos=lambda wildcards: config[samples["type"][wildcards.sample]]["positive"]["R1_trailing"],
    R1_trailing_neg=lambda wildcards: config[samples["type"][wildcards.sample]]["negative"]["R1_trailing"]
  shell: """
        cutadapt -j {threads} \
          -A "{params.R2_trailing_pos};min_overlap=6;max_error_rate=0.1" \
          -A "{params.R2_trailing_neg};min_overlap=6;max_error_rate=0.1" \
          -a "{params.R1_trailing_pos};min_overlap=6;max_error_rate=0.1" \
          -a "{params.R1_trailing_neg};min_overlap=6;max_error_rate=0.1" \
          -o {output.R1} -p {output.R2} {input.R1} {input.R2} > {log}
        """


  # make a size selection before mapping
rule filter_reads:
  input: 
    R1=rules.trim_reads.output.R1, 
    R2=rules.trim_reads.output.R2
  output:  
    R1="02-filtering/{sample}_R1.UMI.ODN.trimmed.filtered.fastq.gz", 
    R2="02-filtering/{sample}_R2.UMI.ODN.trimmed.filtered.fastq.gz",
    R1short=temp("02-filtering/{sample}_R1.UMI.ODN.trimmed.tooshort.fastq.gz"), 
    R2short=temp("02-filtering/{sample}_R2.UMI.ODN.trimmed.tooshort.fastq.gz")
  threads: 6
  log: "logs/{sample}.filter.log"
  params: length=config["minLength"]
  conda: "GENETHOFF"
  message: "remove pairs if one mate is shorter than x bp"
  shell: """
        cutadapt -j {threads} \
          --pair-filter=any \
          --minimum-length {params.length} \
          --too-short-output {output.R1short} \
          --too-short-paired-output  {output.R2short} \
          --output {output.R1} \
          --paired-output {output.R2} {input.R1} {input.R2} > {log}
        """


  # map reads on the reference genome as pairs
if (config["aligner"]  == "bowtie2" or config["aligner"]  == "Bowtie2") :
    rule alignOnGenome:
        input: 
          R1=rules.filter_reads.output.R1,
          R2=rules.filter_reads.output.R2
        output: 
          sam=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sam"),
          R2_unmapped="03-align/{sample}_R2.UMI.ODN.trimmed.unmapped.fastq.gz"
        threads: 6
        log: "logs/{sample}.UMI.ODN.trimmed.filtered.align.log"
        conda: "GENETHOFF"
        message: "Aligning PE reads on genome with Bowtie2"
        params: 
          index=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["index"],
          minFragLength=config["minFragLength"],
          maxFragLength=config["maxFragLength"]
        shell: """
                bowtie2 -p {threads} --no-unal \
                  -I {params.minFragLength} \
                  -X {params.maxFragLength} \
                  --dovetail \
                  --no-mixed \
                  --no-discordant \
                  --un-conc-gz 03-align/{wildcards.sample}_R%.UMI.ODN.trimmed.unmapped.fastq.gz   \
                  -x {params.index} \
                  -1 {input.R1} -2 {input.R2} -S {output.sam} 2> {log}
            """

elif (config["aligner"]  == "bwa") :
    rule alignOnGenome:
        input: 
          R1=rules.filter_reads.output.R1,
          R2=rules.filter_reads.output.R2
        output: 
          sam=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sam"),
          unfilterdsam=temp("03-align/{sample}.UMI.ODN.trimmed.unfiltered.sam")
        threads: 6
        log: "logs/{sample}.UMI.ODN.trimmed.filtered.align.log"
        conda: "GENETHOFF"
        message: "Aligning PE reads on genome with BWA mem"
        params:  index=config["genome"][lambda wildcards:samples["Genome"][wildcards.sample]]["index"]
        shell: """
                #bwa mem -t {threads} {params.index} {input.R1} {input.R2} > {output.sam} 2> {log}
                bwa mem -t {threads} {params.index} {input.R1} {input.R2} > {output.unfilterdsam} 2> {log}
                samtools view -F 0x4 -F 0x8 -F 0x100 -F 0x800 -f 0x2 -b {output.unfilterdsam} > {output.sam}
            """
  
  ## true multihits have MAPQ=1 with bowtie2
  
rule filter_alignments:
    input: 
      sam=rules.alignOnGenome.output.sam
    output: 
      list=temp("03-align/{sample}_multi.txt"),
    threads: 2
    conda: "GENETHOFF"
    message: "keep only alignments with high MAPQ or with equal score with best secondary alignment (multihits)"
    params: minMAPQ=config["minMAPQ"]
    shell: """
    # https://biofinysics.blogspot.com/2014/05/how-does-bowtie2-assign-mapq-scores.html#bt2expt 
        samtools view  {input} -e '((mapq ==1 && [AS] == [XS]) || (mapq >={params.minMAPQ}) || (mapq == 6))' | cut -f1 | sort | uniq  > {output.list}
    """



# sort alignments by names (required for BEDPE conversion) and position (for viewing)
rule sort_aligned:
    input: 
      sam=rules.alignOnGenome.output.sam, 
      list = rules.filter_alignments.output.list
    output:  
      bam="03-align/{sample}.UMI.ODN.trimmed.filtered.sorted.filtered.bam",
      bamPos=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sorted.bam"),
      bamName=temp("03-align/{sample}.UMI.ODN.trimmed.filtered.sortedName.filtered.bam")
    threads: 6
    conda: "GENETHOFF"
    message: "Sort reads by name"
    shell: """
        samtools sort -@ {threads} {input.sam}  > {output.bamPos}

        samtools view -hb --qname-file {input.list}  {output.bamPos} > {output.bam}
        samtools index {output.bam}
        samtools sort -n -@ {threads} {output.bam} > {output.bamName}
    """



# convert BAM file to BEDPE and identify insertion point, read length
# filter alignment with MAPQ score > threshold in config file
# aggregate reads per fragment size and genomic coordinates
# add a cluster ID to group close IS (distance defined in the config file) 
rule call_IS:
    input: 
      rules.sort_aligned.output.bamName
    output: 
      tmp=temp("04-IScalling/{sample}.pebed"), 
      umi="04-IScalling/{sample}.reads_per_UMI_per_IS.bed"
    threads: 1
    conda: "GENETHOFF"
    params: UMI=config["UMI_pattern"]
    shell: """
    
        UMI_length=$(expr length {params.UMI})

        bedtools bamtobed -bedpe -mate1 -i {input} > {output.tmp}
        
        # read R2 contains the genome/ODN junction (use $10 of PE bed = mate 2)
        
        ##################################################################
        # count number of reads per UMI and per IS (pos and strand separated)
        ##################################################################

        awk 'BEGIN{{OFS="\\t";FS="\\t"}} ($1 == $4) {{split($7,a,"_"); if($10=="+") print $4,$5,$5,a[1],a[2],a[3],$8,$10,$3-$5; else print $4,$6-1,$6-1,a[1],a[2],a[3],$8,$10,$6-$2}}' {output.tmp} | sort -k1,1 -k2,3n -k6,6 -k5,5 -k8,8 | bedtools groupby -g 1,2,3,6,5,8 -c 4,7 -o count_distinct,median  > {output.umi}
        
        """


############ RESCUE R2 READS ###################
rule rescue_R2:
  input: 
   R2_short=rules.filter_reads.output.R2short,
   R2_unmapped=rules.alignOnGenome.output.R2_unmapped
  output: 
    sam=temp("03-align/{sample}_R2rescued.UMI.ODN.trimmed.filtered.sam")
  threads: 6
  conda: "GENETHOFF"
  log: "logs/{sample}_R2rescued.UMI.ODN.trimmed.filtered.align.log"
  params: index=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["index"]
  shell: """
    bowtie2 -p {threads} --no-unal \
    -x {params.index} \
    -U {input.R2_short} {input.R2_unmapped} -S {output.sam} 2> {log}
      """

rule sort_rescued_R2:
  input: rules.rescue_R2.output
  output: "03-align/{sample}_R2rescued.UMI.ODN.trimmed.filtered.sorted.bam"
  conda: "GENETHOFF"
  threads:6
  params: minMAPQ=config["minMAPQ"]
  shell: """
    samtools view -b -h  {input} -e '((mapq ==1 && [AS] == [XS]) || (mapq >={params.minMAPQ}))' | samtools sort - > {output}
    samtools index {output}
  """

rule callIS_R2rescued:
  input: rules.sort_rescued_R2.output
  output: 
    umi="04-IScalling/{sample}_R2rescued.reads_per_UMI_per_IS.bed", 
    tmp=temp("04-IScalling/{sample}_rescueR2.bed")
  threads: 1
  conda: "GENETHOFF"
  params: UMI=config["UMI_pattern"]
  shell: """
  
      UMI_length=$(expr length {params.UMI})

      bedtools bamtobed -i {input} > {output.tmp}
      
      # read R2 contains the genome/ODN junction
      
      ##################################################################
      # count number of reads per UMI and per IS (pos and strand separated)
      ##################################################################

      awk 'BEGIN{{OFS="\\t";FS="\\t"}} {{split($4,a,"_"); if($6=="+") print $1,$2,$2,a[1],a[2],a[3],$5,$6,$3-$2; else print $1,$3-1,$3-1,a[1],a[2],a[3],$5,$6,$3-$2}}' {output.tmp} | sort -k1,1 -k2,3n -k6,6 -k5,5 -k8,8 | bedtools groupby -g 1,2,3,6,5,8 -c 4,7 -o count_distinct,median  > {output.umi}
      
      """

#######################

rule correct_UMI:
    input: 
      bed=rules.call_IS.output.umi, 
      bed_rescued=rules.callIS_R2rescued.output.umi
    output: 
      "04-IScalling/{sample}.reads_per_UMI_per_IS_corrected.bed"
    conda: "GENETHOFF"
    threads: 2
    params: hamming=config["UMI_hamming_distance"], filter = config["UMI_filter"],  UMI=config["UMI_pattern"], method = config["UMI_deduplication"], rescueR2=config["rescue_R2"]
    shell: """
        Rscript {workflow.source_path("../00-pipeline/correct_umi.R")} {input.bed} {params.UMI} {params.filter} {params.hamming} {params.method} {output} {params.rescueR2}
        """


rule Collapse_UMI_IS:
    input: 
      rules.correct_UMI.output
    output: 
      collapse="04-IScalling/{sample}.UMIs_per_IS_in_Cluster.bed"
    threads: 1
    conda: "GENETHOFF"
    params: window=config["ISbinWindow"], minMAPQ=config["minMAPQ"],minReadsPerUMI=config["minReadsPerUMI"],minUMIPerIS=config["minUMIPerIS"]
    shell: """
        
        ##################################################################
        # aggregate UMI per IS
        ##################################################################
        
        awk 'NR>1 && $7>{params.minReadsPerUMI}' {input} | sort -k1,1 -k2,2n -k3,3n -k5,5 -k 6,6 | bedtools groupby -g 1,2,3,5,6 -c 4,7,4,7,8  -o count_distinct,sum,collapse,collapse,median  | awk 'BEGIN{{OFS="\\t";FS="\\t"}} $6>{params.minUMIPerIS} {{print $1,$2,$3,"ID_"NR,$10,$5,$4,$6,$7,$8,$9}}' |   sort -k1,1 -k2,2n -k3,3n -k5,5 | bedtools cluster -d {params.window} > {output.collapse}
        """
    




    
rule get_fasta_around_is:
    input: 
      bed=rules.Collapse_UMI_IS.output.collapse,
      length=["../02-ressources/{genome}.chrom.length".format(genome=genome) for genome in genomes_unique]
    output: 
      cluster="04-IScalling/{sample}.cluster_slop.bed",
      fa="04-IScalling/{sample}.cluster_slop.fa"
    threads: 1
    conda: "GENETHOFF"
    params: fasta=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["fasta"], 
     chr_length=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["fasta"]+".fai",
     slop_size=config["slopSize"]
    shell: """
        # echo -e "#chromosome\tstart\tend\tclusterID\tN_orientations\tmedianMAPQ\tN_IS\tN_UMI\tN_reads" > {output.cluster}
        
        sort -k12,12n -k7 -k6 {input.bed} | bedtools groupby -g 12 -c 1,2,3,4,5,6,7,8,9 -o distinct,min,max,count_distinct,median,count_distinct,count_distinct,sum,sum |  awk 'BEGIN{{OFS="\\t"}}{{print $2,$3,$4,$1,$6,$5,$7,$8,$9,$10}}' | bedtools slop -i - -b {params.slop_size} -g {params.chr_length} > {output.cluster} 
        
        ## ORDER of columns
        ## chr start end clusterID MapQmedian nIS nOrientations nPCR nUMI, nReads
        
        
        bedtools getfasta -name -fi {params.fasta} -bed {output.cluster} > {output.fa}
        """ 

rule get_stats_fq:
    input: 
     rules.merge_samples_i2.output.R1,
     rules.merge_samples_i2.output.R2,
     rules.trim_ODN.output.R1,
     rules.trim_ODN.output.R2,
     rules.filter_reads.output.R1,
     rules.filter_reads.output.R2,
    output: "05-Report/{sample}.stat"
    conda: "GENETHOFF"
    threads: 6
    shell: """
        seqkit stat -j {threads} -a -T {input} > {output}
        """

def get_output_files(wildcards):
    # Custom logic to determine output file pattern
    if wildcards.side == "3":
        return "06-offPredict/{gen}_{side}_{grna}_{pam}.csv".format(gen=wildcards.gen, grna=wildcards.grna, pam=wildcards.pam,side=wildcards.side)
    else:
        return "06-offPredict/{gen}_{side}_{pam}_{grna}.csv".format(gen=wildcards.gen, grna=wildcards.grna, pam=wildcards.pam,side=wildcards.side)




rule predict_offtarget_SWOffinder:
    input: 
    output: csv= "06-offPredict/{gen}_{grna}_{pam}_{side}.csv"
    conda: "GENETHOFF"
    params: genome=lambda wildcards: config["genome"][wildcards.gen]["fasta"],
        gRNA=lambda wildcards:{wildcards.grna},
        maxE=config["max_edits_crRNA"],               #Max edits allowed (integer).
        maxM=config["max_edits_crRNA"],               #Max mismatches allowed without bulges (integer).
        maxMB=config["SWoffFinder"]["maxMB"],             #Max mismatches allowed with bulges (integer).
        maxB=config["SWoffFinder"]["maxB"],               #Max bulges allowed (integer).
        window_size=config["SWoffFinder"]["window_size"], #The window size for choosing the best in a window
        bulges=config["tolerate_bulges"].upper(),
        PAM=lambda wildcards:wildcards.pam,
        PAM_side=lambda wildcards:wildcards.side,
        SWoffFinder=config["SWoffFinder"]["path"]
    threads: 12
    shell: """
        if [ {params.bulges} = TRUE ] ; 
        then 
            bulge_size={params.maxB}; 
        else 
            bulge_size=0;
        fi
    
    
        if [ {params.PAM_side} = "5" ];
        then
            sequence={params.PAM}{params.gRNA}
            java -cp {params.SWoffFinder}/bin SmithWatermanOffTarget.SmithWatermanOffTargetSearchAlign {params.genome} $sequence 06-offPredict/{wildcards.gen}_{wildcards.grna}_{wildcards.pam}_{wildcards.side} {params.maxE} {params.maxM} {params.maxMB} $(echo $bulge_size) {threads} TRUE {params.window_size} {params.PAM} TRUE
        else
            sequence={params.gRNA}{params.PAM}
            java -cp {params.SWoffFinder}/bin SmithWatermanOffTarget.SmithWatermanOffTargetSearchAlign {params.genome} $sequence 06-offPredict/{wildcards.gen}_{wildcards.grna}_{wildcards.pam}_{wildcards.side} {params.maxE} {params.maxM} {params.maxMB} $(echo $bulge_size) {threads} TRUE {params.window_size} {params.PAM} TRUE
        fi
        
        """

rule report_data:
    input: fasta=rules.get_fasta_around_is.output.fa, cluster=rules.get_fasta_around_is.output.cluster, bed=rules.Collapse_UMI_IS.output.collapse, statfq=rules.get_stats_fq.output,statal=rules.alignOnGenome.log
    output: "05-Report/{sample}.rdata"
    conda: "GENETHOFF"
    threads: 4
    params: gRNA_seq=lambda wildcards:samples["gRNA_sequence"][wildcards.sample], 
     gRNA_name=lambda wildcards:samples["gRNA_name"][wildcards.sample],
     PAM=lambda wildcards:samples["PAM_sequence"][wildcards.sample],
     offset=lambda wildcards:samples["Cut_Offset"][wildcards.sample],
     max_edits=config["max_edits_crRNA"],
     bulges=config["tolerate_bulges"],
     pam_side=lambda wildcards:samples["PAM_side"][wildcards.sample]
    shell : """
        Rscript ../00-pipeline/multiple_alignments.R {input.fasta} {input.cluster} {input.bed} {params.gRNA_seq} {params.gRNA_name}  {params.PAM}  {params.offset} {params.max_edits} {params.bulges} {params.pam_side} {output}
        """        
        

rule annotate_sites:
    input: rdata=rules.report_data.output, annot=["../02-ressources/{genome}.rds".format(genome=genome) for genome in genomes_unique]
    output: "results/{sample}_summary.xlsx"
    conda: "GENETHOFF"
    threads: 4
    params: species=lambda wildcards: samples["Genome"][wildcards.sample], oncoList=lambda wildcards: config["genome"][samples["Genome"][wildcards.sample]]["oncogene_list"]
    shell: """
        Rscript ../00-pipeline/annotate_cuting_sites.R {params.species} {input.rdata} {output} {params.oncoList}
        """


rule report:
    input: summaries= ["results/{sample}_summary.xlsx".format(sample=sample) for sample in samples_unique],
     predictions=expand("06-offPredict/{gen}_{grna}_{pam}_{side}.csv", zip, grna=gRNAs, gen=genomes,pam=PAMs,side=sides)
    output: report_rdata= "results/report.rdata"
    conda: "GENETHOFF"
    threads: 1
    params: sampleInfo=config["sampleInfo_path"], 
     config=os.path.abspath(workflow.configfiles[0]), 
     minUMI_alignments_figure=config["minUMI_alignments_figure"],
     min_predicted_distance=config["min_predicted_distance"],
     max_clusters=config["max_clusters"]
    shell: """
        mkdir -p results/report-files/;
        Rscript ../00-pipeline/generate_tables.r "{input.summaries}" {params.sampleInfo} {params.config} "{input.predictions}" {params.max_clusters} {params.minUMI_alignments_figure} {params.min_predicted_distance}
        """
      
rule make_report:
    input:  rules.report.output.report_rdata
    output: report_html="results/"+os.path.basename(os.getcwd())+"_report.html",report_pdf="results/"+os.path.basename(os.getcwd())+"_report.pdf"
    conda: "GENETHOFF"
    params: config_path=config_file_path
    threads: 1
    shell: """
        Rscript ../00-pipeline/publish_report.r {input} {params.config_path} {output.report_html}
        Rscript ../00-pipeline/publish_report_pdf.r {input} {params.config_path} {output.report_pdf}
    """
onsuccess:
    print("Workflow finished, no error")
    shell("conda run -n GENETHOFF echo 'You rock baby !' | cowpy -e greedy")
    if config["clean_intermediates_files"] == "TRUE":
      shell("rm -r 0*/")

onerror:
    print("An error occurred")
    shell("conda run -n GENETHOFF echo 'Houston, we have a problem' | cowpy -e dead -c mutilated")

